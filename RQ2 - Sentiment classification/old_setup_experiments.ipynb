{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beren\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:44: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\beren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\beren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\beren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2268\\3438650758.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;31m# Train Word2Vec model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparlvote\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'speech_processed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparlvote\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'speech_processed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparlvote\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'speech_processed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m                 end_alpha=self.min_alpha, compute_loss=self.compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    431\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1071\u001b[0m                     \u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m                     \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1073\u001b[1;33m                     callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[0;32m   1074\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m   1431\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0;32m   1432\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1433\u001b[1;33m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_corpus_file_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1434\u001b[0m         )\n\u001b[0;32m   1435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1286\u001b[1;33m             \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocks if workers too slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1287\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# a thread reporting that it finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.7_3.7.2544.0_x64__qbz5n2kfra8p0\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.7_3.7.2544.0_x64__qbz5n2kfra8p0\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from afinn import Afinn\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from itertools import product\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from transformers import BertTokenizer\n",
    "from itertools import product\n",
    "import csv\n",
    "\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras_tuner import GridSearch as kt\n",
    "\n",
    "#Import data\n",
    "parlvote = pd.read_csv('ParlVote_concat.csv', header=None)\n",
    "first_row = parlvote.iloc[0].tolist()\n",
    "parlvote.columns = first_row\n",
    "parlvote = parlvote.drop(index=0)\n",
    "\n",
    "#########################BEGIN SAMPLE##################################\n",
    "#parlvote =  parlvote.sample(n=300, random_state=42)\n",
    "##########################END SAMPLE###################################\n",
    "\n",
    "### Train for domain specific lexion ####\n",
    "# Preprocess\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the set of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize and lowercase\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token.isalpha()]\n",
    "\n",
    "# Apply preprocessing to the 'speech' column and store the result in 'speech_processed'\n",
    "parlvote['speech_processed'] = parlvote['speech'].apply(preprocess_text)\n",
    "\n",
    "parlvote['speech_svm'] = parlvote['speech_processed'].apply(' '.join)\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(parlvote['speech_processed'], vector_size=300, window=6, min_count=10, workers=4)\n",
    "model.train(parlvote['speech_processed'], total_examples=len(parlvote['speech_processed']), epochs=5)\n",
    "\n",
    "######## REMOVE speeches with >512 tokens ##########\n",
    "rem_token = []\n",
    "sh_pv_before = parlvote.shape\n",
    "\n",
    "## Instantiate a BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "# Tokenize the speeches and count tokens\n",
    "parlvote['token_length'] = parlvote['speech'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "\n",
    "# Filter out rows with more than 512 tokens\n",
    "max_tokens = 512\n",
    "parlvote = parlvote[parlvote['token_length'] <= max_tokens]\n",
    "\n",
    "# Now 'filtered_parlvote' is a DataFrame containing only rows with no more than 512 BERT tokens in the 'speech' column\n",
    "# Shape after removing neutral\n",
    "sh_pv_after = parlvote.shape\n",
    "rem_token.append({\"original\" : sh_pv_before, \">512 removed\" : sh_pv_after})\n",
    "rem_token_df = pd.DataFrame(rem_token)\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "rem_token_df.to_csv(\"removed_512.csv\", index=False)\n",
    "\n",
    "\n",
    "##### GENERATE SENTIMENT LABELS ########\n",
    "### Domain Specific ####\n",
    "\n",
    "\n",
    "def sentiment_score(speech, model, positive_seeds, negative_seeds):\n",
    "    # Tokenize the speech\n",
    "    words = speech.split()\n",
    "    \n",
    "    score = 0\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            word_vector = model.wv[word]\n",
    "            \n",
    "            # Sum of cosine similarities with positive seed words\n",
    "            for seed in positive_seeds:\n",
    "                if seed in model.wv:\n",
    "                    seed_vector = model.wv[seed]\n",
    "                    score += np.dot(word_vector, seed_vector) / (np.linalg.norm(word_vector) * np.linalg.norm(seed_vector))\n",
    "                    \n",
    "            # Subtract sum of cosine similarities with negative seed words\n",
    "            for seed in negative_seeds:\n",
    "                if seed in model.wv:\n",
    "                    seed_vector = model.wv[seed]\n",
    "                    score -= np.dot(word_vector, seed_vector) / (np.linalg.norm(word_vector) * np.linalg.norm(seed_vector))\n",
    "    \n",
    "    return score\n",
    "\n",
    "positive_seeds = [\"good\", \"excellent\", \"correct\", \"best\", \"happy\", \"positive\", \"fortunate\"]\n",
    "negative_seeds = [\"bad\", \"terrible\", \"wrong\", \"worst\", \"disappointed\", \"negative\", \"unfortunate\"]\n",
    "\n",
    "# Generate raw sentiment scores\n",
    "raw_sentiment_scores = [sentiment_score(speech, model, positive_seeds, negative_seeds) for speech in parlvote['speech']]\n",
    "\n",
    "# Reshape the scores to fit the scaler input\n",
    "raw_sentiment_scores = np.array(raw_sentiment_scores).reshape(-1, 1)\n",
    "\n",
    "# Create a MinMaxScaler object with feature range of -1 to 1\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit the scaler to the data and transform the data\n",
    "scaled_sentiment_scores = scaler.fit_transform(raw_sentiment_scores)\n",
    "\n",
    "# Create 'domain_sentiment' column in the parlvote dataframe\n",
    "parlvote['domain_sentiment'] = scaled_sentiment_scores.flatten()\n",
    "\n",
    "\n",
    "###### BREAK #########\n",
    "# if 1 == 1:\n",
    "#     raise ValueError(\"Stopping execution at this cell\")\n",
    "\n",
    "\n",
    "### Standard Lexicons ###\n",
    "\n",
    "# Initialize sentiment analyzers\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "afinn_analyzer = Afinn()\n",
    "textblob_analyzer = TextBlob\n",
    "\n",
    "# Define a function to generate the sentiment scores for each row\n",
    "def generate_sentiment_scores(row):\n",
    "    text = row['speech']\n",
    "    \n",
    "    # Generate VADER sentiment score\n",
    "    vader_scores = vader_analyzer.polarity_scores(text)['compound']\n",
    "    \n",
    "    # Generate Afinn sentiment score\n",
    "    afinn_score = afinn_analyzer.score(text)\n",
    "    \n",
    "    # Generate TextBlob sentiment score\n",
    "    textblob_score = textblob_analyzer(text).sentiment.polarity\n",
    "    \n",
    "    return pd.Series([vader_scores, textblob_score, afinn_score])\n",
    "\n",
    "\n",
    "# Apply the function to generate sentiment scores for each row and store the scores in new columns\n",
    "parlvote[['vader', 'textblob', 'afinn']] = parlvote.apply(generate_sentiment_scores, axis=1)\n",
    "\n",
    "\n",
    "# Define a function to convert VADER sentiment scores to labels\n",
    "def vader_score_to_label(score):\n",
    "    if score > 0.05:\n",
    "        return 'Positive'\n",
    "    elif score < -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Define a function to convert TextBlob sentiment scores to labels\n",
    "def textblob_score_to_label(score):\n",
    "    if score < 0:\n",
    "        return 'Negative'\n",
    "    elif score == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "# Define a function to convert Afinn sentiment scores to labels\n",
    "def afinn_score_to_label(score):\n",
    "    if score > 0:\n",
    "        return 'Positive'\n",
    "    elif score < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "\n",
    "# Apply the label conversion functions to the sentiment score columns\n",
    "parlvote['vader'] = parlvote['vader'].apply(vader_score_to_label)\n",
    "parlvote['textblob'] = parlvote['textblob'].apply(textblob_score_to_label)\n",
    "parlvote['afinn'] = parlvote['afinn'].apply(afinn_score_to_label)\n",
    "parlvote['domain_sentiment'] = parlvote['domain_sentiment'].apply(textblob_score_to_label)\n",
    "\n",
    "######### REMOVE 'NEUTRAL' LABELS #########\n",
    "# Shape before removing neutral\n",
    "removed = []\n",
    "before = parlvote.shape\n",
    "\n",
    "# remove instances where a row contains 'neutral'\n",
    "pv = parlvote.loc[~((parlvote['vader'] == 'Neutral') | (parlvote['textblob'] == 'Neutral') | (parlvote['afinn'] == 'Neutral') | (parlvote['domain_sentiment'] == 'Neutral'))]\n",
    "# Create a copy of the DataFrame to avoid modifying the original DataFrame\n",
    "pv = pv.copy()\n",
    "\n",
    "# Shape after removing neutral\n",
    "after = pv.shape\n",
    "removed.append({\"original\" : before, \"neutral removed\" : after})\n",
    "removed_df = pd.DataFrame(removed)\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "removed_df.to_csv(\"removed_neutral.csv\", index=False)\n",
    "\n",
    "\n",
    "######## CHANGE TO BINARY #########\n",
    "def label_to_binary(label):\n",
    "    if label == 'Positive':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def str_to_int(label):\n",
    "    if label == '1':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Convert sentiment labels to binary values for 'vader', 'textblob', and 'afinn' columns\n",
    "pv['vader'] = pv['vader'].apply(label_to_binary)\n",
    "pv['textblob'] = pv['textblob'].apply(label_to_binary)\n",
    "pv['afinn'] = pv['afinn'].apply(label_to_binary)\n",
    "pv['domain_sentiment'] = pv['domain_sentiment'].apply(label_to_binary)\n",
    "\n",
    "# Convert the string of 'vote' to integer\n",
    "pv['vote'] = pv['vote'].apply(str_to_int)\n",
    "\n",
    "\n",
    "####### EMBEDDINGS ########\n",
    "# Load a pre-trained BERT model\n",
    "embedding_model = SentenceTransformer('bert-base-cased')\n",
    "# Generate embeddings for the 'speech' column\n",
    "pv['embeddings'] = parlvote['speech'].apply(lambda x: embedding_model.encode(x, convert_to_numpy=True))\n",
    "print(\"Shape of the first embedding: \", pv['embeddings'].iloc[0].shape)\n",
    "# Now 'parlvote' has a new column 'embeddings' containing the embeddings as numpy arrays\n",
    "\n",
    "\n",
    "# # Define the label columns\n",
    "# label_columns = [\"vote\", \"vader\", \"textblob\", \"afinn\", \"domain_sentiment\"]\n",
    "\n",
    "# # Initialize an empty dictionary to store the distributions\n",
    "# distributions = {}\n",
    "\n",
    "# # For each label column, compute the distribution and store it in the dictionary\n",
    "# for label in label_columns:\n",
    "#     distributions[label] = pv[label].value_counts(normalize=True) * 100\n",
    "\n",
    "# # Convert the distributions dictionary into a DataFrame\n",
    "# distributions_df = pd.DataFrame(distributions)\n",
    "\n",
    "# # Transpose the DataFrame so that each row corresponds to a label\n",
    "# distributions_df = distributions_df.transpose()\n",
    "\n",
    "# # Save the distributions DataFrame to a CSV file\n",
    "# distributions_df.to_csv(\"distributions.csv\")\n",
    "\n",
    "\n",
    "###### ONE-HOT ENCODE PV (train-val) AND PV_TEST #######\n",
    "# Define a function to one-hot encode a column\n",
    "def one_hot_encode(df, column):\n",
    "    df[column + '_positive'] = (df[column] == 1).astype(int)\n",
    "    df[column + '_negative'] = (df[column] == 0).astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply the function to one-hot encode the columns in the 'pv' dataframe\n",
    "pv = one_hot_encode(pv, 'vote')\n",
    "pv = one_hot_encode(pv, 'vader')\n",
    "pv = one_hot_encode(pv, 'textblob')\n",
    "pv = one_hot_encode(pv, 'afinn')\n",
    "pv = one_hot_encode(pv, 'domain_sentiment')\n",
    "\n",
    "# # Apply the function to one-hot encode the columns in the 'pv_test' dataframe\n",
    "# pv_test = one_hot_encode(pv_test, 'vote')\n",
    "# pv_test = one_hot_encode(pv_test, 'vader')\n",
    "# pv_test = one_hot_encode(pv_test, 'textblob')\n",
    "# pv_test = one_hot_encode(pv_test, 'afinn')\n",
    "# pv_test = one_hot_encode(pv_test, 'domain_sentiment')\n",
    "\n",
    "# # Prepare sliding window and sentence embedding functions\n",
    "# def sliding_window(text, window_size, stride):\n",
    "#     num_chunks = max((len(text) - window_size) // stride + 1, 1)\n",
    "#     chunks = [text[i*stride:i*stride + window_size] for i in range(num_chunks)]\n",
    "#     return chunks\n",
    "\n",
    "\n",
    "# def get_sentence_embeddings(embedding_model, data, max_length, window_size, stride, aggregation='mean'):\n",
    "#     all_embeddings = []\n",
    "    \n",
    "#     for text in data:\n",
    "#         if len(text) > max_length:\n",
    "#             chunks = sliding_window(text, window_size, stride)\n",
    "#         else:\n",
    "#             chunks = [text]\n",
    "\n",
    "#         # Generate embeddings for each chunk\n",
    "#         chunk_embeddings = embedding_model.encode(chunks)\n",
    "        \n",
    "#         # Aggregate the embeddings\n",
    "#         if aggregation == 'mean':\n",
    "#             aggregated_embedding = np.mean(chunk_embeddings, axis=0)\n",
    "#         elif aggregation == 'max':\n",
    "#             aggregated_embedding = np.max(chunk_embeddings, axis=0)\n",
    "#         elif aggregation == 'min':\n",
    "#             aggregated_embedding = np.min(chunk_embeddings, axis=0)\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unsupported aggregation method: {aggregation}\")\n",
    "\n",
    "#         all_embeddings.append(aggregated_embedding)\n",
    "\n",
    "#     return np.array(all_embeddings)\n",
    "\n",
    "# # Set the parameters for the sliding window\n",
    "# max_length = 512\n",
    "# window_size = 400\n",
    "# stride = 200\n",
    "\n",
    "def print_majority_baselines(df, one_hot_encoded_columns):\n",
    "    for columns in one_hot_encoded_columns:\n",
    "        majority_class = df[columns].sum(axis=0).idxmax()\n",
    "        majority_baseline = df[majority_class].mean()\n",
    "        \n",
    "        print(f\"Majority baseline for {', '.join(columns)}:\")\n",
    "        print(f\"  Majority class: {majority_class}\")\n",
    "        print(f\"  Baseline accuracy: {majority_baseline:.2%}\\n\")\n",
    "\n",
    "# usage\n",
    "one_hot_encoded_columns = [\n",
    "    ['vote_positive', 'vote_negative'],\n",
    "    ['vader_positive', 'vader_negative'],\n",
    "    ['textblob_positive', 'textblob_negative'],\n",
    "    ['afinn_positive', 'afinn_negative'], \n",
    "    ['domain_sentiment_positive', 'domain_sentiment_negative']\n",
    "]\n",
    "##### PRINT MAJORITY BASELINES FOR PV ######\n",
    "print_majority_baselines(pv, one_hot_encoded_columns)\n",
    "\n",
    "#### PRINT MAJORITY BASELINES FOR PV_TEST #####\n",
    "def majority_baseline(df, columns):\n",
    "        majority_class = df[columns].sum(axis=0).idxmax()\n",
    "        majority_baseline = df[majority_class].mean()\n",
    "        return majority_baseline\n",
    "\n",
    "vote_majority = majority_baseline(pv, one_hot_encoded_columns[0])\n",
    "vader_majority = majority_baseline(pv, one_hot_encoded_columns[1])\n",
    "textblob_majority = majority_baseline(pv, one_hot_encoded_columns[2])\n",
    "afinn_majority = majority_baseline(pv, one_hot_encoded_columns[3])\n",
    "domain_majority = majority_baseline(pv, one_hot_encoded_columns[4])\n",
    "\n",
    "# Convert one-hot encoded columns to a NumPy array\n",
    "vote_labels = pv[['vote_positive', 'vote_negative']].values\n",
    "vader_labels = pv[['vader_positive', 'vader_negative']].values\n",
    "textblob_labels = pv[['textblob_positive', 'textblob_negative']].values\n",
    "afinn_labels = pv[['afinn_positive', 'afinn_negative']].values\n",
    "domain_labels = pv[['domain_sentiment_positive', 'domain_sentiment_negative']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_pipeline(data, train_labels, test_labels, lstm_units, spatial_dropout_rate, name, dense_units, learning_rate, batch_size, epochs, input_dropout_rate, recurrent_dropout_rate, output_dropout_rate, smote, use_kfold, test_size=0.2, n_splits=5):\n",
    "    data = np.stack(data, axis=0)  # Convert the lists of embeddings to a NumPy array\n",
    "    \n",
    "    if use_kfold:\n",
    "        # K-fold cross-validation\n",
    "        kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        splits = kfold.split(data, np.argmax(train_labels, axis=1))\n",
    "    else:\n",
    "        train_data, test_data, train_labels, _ = train_test_split(data, train_labels, test_size=test_size, stratify=vote_labels, random_state=42)\n",
    "        test_labels = test_labels[len(train_labels):]\n",
    "        splits = [(range(len(train_data)), range(len(test_data)))]\n",
    "\n",
    "    # Store evaluation scores for each fold or train-test split\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_index, test_index in splits:\n",
    "        X_train, X_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = train_labels[train_index], test_labels[test_index]\n",
    "\n",
    "        # Create the SMOTE instance if specified\n",
    "        if smote:\n",
    "            smote_sampler = SMOTE(random_state=42)\n",
    "            X_train, y_train = smote_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Define the model\n",
    "        model = models.Sequential([\n",
    "            layers.Input(shape=(X_train.shape[1],)),\n",
    "            layers.Reshape((1, X_train.shape[1])),\n",
    "            layers.SpatialDropout1D(spatial_dropout_rate),\n",
    "            layers.Bidirectional(layers.LSTM(lstm_units, dropout=input_dropout_rate, recurrent_dropout=recurrent_dropout_rate)),\n",
    "            layers.Dense(dense_units, activation='relu'),\n",
    "            layers.Dropout(output_dropout_rate),\n",
    "            layers.Dense(y_train.shape[1], activation='softmax')\n",
    "        ])\n",
    "\n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "        # Predict on test data\n",
    "        y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "        y_test_decoded = np.argmax(y_test, axis=-1)\n",
    "\n",
    "        train_val_name = 'Train-Validation Loss: ' + name\n",
    "        # Plot training and validation loss\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(train_val_name)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Compute the confusion matrix\n",
    "        confusion_mtx = confusion_matrix(y_test_decoded, y_pred)\n",
    "\n",
    "        cf_name = 'Confusion Matrix: ' + name \n",
    "         # Plot the confusion matrix\n",
    "        plt.figure(figsize=(10,8))\n",
    "        sns.heatmap(confusion_mtx, annot=True, fmt=\"d\", cmap='Blues')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.ylabel('True label')\n",
    "        plt.title(cf_name)\n",
    "        plt.show()\n",
    "\n",
    "        # Print the classification report\n",
    "        print(classification_report(y_test_decoded, y_pred))\n",
    "\n",
    "        pr_name = 'Precision-Recall curve: ' + name\n",
    "        # Plot Precision-Recall curve\n",
    "        precision, recall, _ = precision_recall_curve(y_test_decoded, y_pred)\n",
    "        plt.figure(figsize=(7,7))\n",
    "        plt.plot(recall, precision, label='PR curve (area = %0.2f)' % auc(recall, precision))\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title(pr_name)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "        # Evaluation\n",
    "        accuracy = accuracy_score(y_test_decoded, y_pred)\n",
    "        precision = precision_score(y_test_decoded, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test_decoded, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test_decoded, y_pred, average='weighted')\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Print average scores\n",
    "    print(f\"Average accuracy: {np.mean(accuracies):.2%}\")\n",
    "    print(f\"Average precision: {np.mean(precisions):.2%}\")\n",
    "    print(f\"Average recall: {np.mean(recalls):.2%}\")\n",
    "    print(f\"Average F1-score: {np.mean(f1_scores):.2%}\")\n",
    "    accuracy = np.mean(accuracies)\n",
    "    precision = np.mean(precisions)\n",
    "    recall = np.mean(recalls)\n",
    "    f1 = np.mean(f1_scores)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "def mlp_baseline_pipeline(data, train_labels, test_labels, smote, use_kfold, name, test_size=0.2, n_splits=5, dense_units=100, learning_rate=1e-3, batch_size=32, epochs=10):\n",
    "    data = np.stack(data, axis=0)  # Convert the lists of embeddings to a NumPy array\n",
    "    if use_kfold:\n",
    "        kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        splits = kfold.split(data, np.argmax(train_labels, axis=1))\n",
    "    else:\n",
    "        train_data, test_data, train_labels, _ = train_test_split(data, train_labels, test_size=test_size, stratify=vote_labels, random_state=42)\n",
    "        test_labels = test_labels[len(train_labels):]\n",
    "        splits = [(range(len(train_data)), range(len(test_data)))]\n",
    "\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_index, test_index in splits:\n",
    "        X_train, X_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = train_labels[train_index], test_labels[test_index]\n",
    "\n",
    "        if smote:\n",
    "            smote_sampler = SMOTE(random_state=42)\n",
    "            X_train, y_train = smote_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        model = Sequential([\n",
    "            Dense(dense_units, input_shape=(X_train.shape[1],)),  # Hidden layer of 100 nodes\n",
    "            BatchNormalization(),  # Batch normalization\n",
    "            Activation('relu'),  # ReLU activation function\n",
    "            Dropout(0.5),  # Dropout regularization rate of 0.5\n",
    "            Dense(y_train.shape[1], activation='sigmoid')  # Sigmoid activation in the output layer\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n",
    "        model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "        y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "        y_test_decoded = np.argmax(y_test, axis=-1)\n",
    "\n",
    "        train_val_name = 'Train-Validation Loss: ' + name\n",
    "        #PLOTTING\n",
    "        # Plot training and validation loss\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(train_val_name)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Compute the confusion matrix\n",
    "        confusion_mtx = confusion_matrix(y_test_decoded, y_pred)\n",
    "        \n",
    "        cf_name = 'Confusion Matrix: ' + name \n",
    "\n",
    "         # Plot the confusion matrix\n",
    "        plt.figure(figsize=(10,8))\n",
    "        sns.heatmap(confusion_mtx, annot=True, fmt=\"d\", cmap='Blues')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.ylabel('True label')\n",
    "        plt.title(cf_name)\n",
    "        plt.show()\n",
    "\n",
    "        # Print the classification report\n",
    "        print(classification_report(y_test_decoded, y_pred))\n",
    "\n",
    "        pr_name = 'Precision-Recall curve: ' + name\n",
    "        # Plot Precision-Recall curve\n",
    "        precision, recall, _ = precision_recall_curve(y_test_decoded, y_pred)\n",
    "        plt.figure(figsize=(7,7))\n",
    "        plt.plot(recall, precision, label='PR curve (area = %0.2f)' % auc(recall, precision))\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title(pr_name)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        #EVALUATION METRICS\n",
    "        accuracy = accuracy_score(y_test_decoded, y_pred)\n",
    "        precision = precision_score(y_test_decoded, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test_decoded, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test_decoded, y_pred, average='weighted')\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    print(f\"Average accuracy: {np.mean(accuracies):.2%}\")\n",
    "    print(f\"Average precision: {np.mean(precisions):.2%}\")\n",
    "    print(f\"Average recall: {np.mean(recalls):.2%}\")\n",
    "    print(f\"Average F1-score: {np.mean(f1_scores):.2%}\")\n",
    "    accuracy = np.mean(accuracies)\n",
    "    precision = np.mean(precisions)\n",
    "    recall = np.mean(recalls)\n",
    "    f1 = np.mean(f1_scores)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "\n",
    "# ##### LOGISTIC REGRESSION #######\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.metrics import average_precision_score\n",
    "\n",
    "# def logistic_regression_pipeline(data, train_labels, test_labels, smote, use_kfold, name, test_size=0.2, n_splits=5):\n",
    "#     data = np.stack(data, axis=0)  # Convert the lists of embeddings to a NumPy array\n",
    "#     le = LabelEncoder()\n",
    "#     train_labels_encoded = le.fit_transform(np.argmax(train_labels, axis=1))\n",
    "#     test_labels_encoded = le.transform(np.argmax(test_labels, axis=1))\n",
    "#     if use_kfold:\n",
    "#         kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "#         splits = kfold.split(data, train_labels_encoded)\n",
    "#     else:\n",
    "#         train_data, test_data, train_labels, _ = train_test_split(data, train_labels_encoded, test_size=test_size, stratify=train_labels_encoded, random_state=42)\n",
    "#         test_labels = test_labels_encoded[len(train_labels):]\n",
    "#         splits = [(range(len(train_data)), range(len(test_data)))]\n",
    "\n",
    "#     accuracies = []\n",
    "#     precisions = []\n",
    "#     recalls = []\n",
    "#     f1_scores = []\n",
    "\n",
    "#     for train_index, test_index in splits:\n",
    "#         X_train, X_test = data[train_index], data[test_index]\n",
    "#         y_train, y_test = train_labels[train_index], test_labels[test_index]\n",
    "\n",
    "#         if smote:\n",
    "#             smote_sampler = SMOTE(random_state=42)\n",
    "#             X_train, y_train = smote_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "#         model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "#         model.fit(X_train, y_train)\n",
    "\n",
    "#         y_pred = model.predict(X_test)\n",
    "\n",
    "#         # Compute the confusion matrix\n",
    "#         confusion_mtx = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "#         cf_name = 'Confusion Matrix: ' + name \n",
    "\n",
    "#          # Plot the confusion matrix\n",
    "#         plt.figure(figsize=(10,8))\n",
    "#         sns.heatmap(confusion_mtx, annot=True, fmt=\"d\", cmap='Blues')\n",
    "#         plt.xlabel('Predicted label')\n",
    "#         plt.ylabel('True label')\n",
    "#         plt.title(cf_name)\n",
    "#         plt.show()\n",
    "\n",
    "#         # Print the classification report\n",
    "#         print(classification_report(y_test, y_pred))\n",
    "\n",
    "#         pr_name = 'Precision-Recall curve: ' + name\n",
    "#         # Plot Precision-Recall curve\n",
    "#         precision, recall, _ = precision_recall_curve(y_test, model.predict_proba(X_test)[:,1])\n",
    "#         plt.figure(figsize=(7,7))\n",
    "#         plt.plot(recall, precision, label='PR curve (area = %0.2f)' % auc(recall, precision))\n",
    "#         plt.xlabel('Recall')\n",
    "#         plt.ylabel('Precision')\n",
    "#         plt.title(pr_name)\n",
    "#         plt.legend(loc=\"lower right\")\n",
    "#         plt.show()\n",
    "\n",
    "#         #EVALUATION METRICS\n",
    "#         accuracy = accuracy_score(y_test, y_pred)\n",
    "#         precision = precision_score(y_test, y_pred, average='weighted')\n",
    "#         recall = recall_score(y_test, y_pred, average='weighted')\n",
    "#         f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "#         accuracies.append(accuracy)\n",
    "#         precisions.append(precision)\n",
    "#         recalls.append(recall)\n",
    "#         f1_scores.append(f1)\n",
    "\n",
    "#     print(f\"Average accuracy: {np.mean(accuracies):.2%}\")\n",
    "#     print(f\"Average precision: {np.mean(precisions):.2%}\")\n",
    "#     print(f\"Average recall: {np.mean(recalls):.2%}\")\n",
    "#     print(f\"Average F1-score: {np.mean(f1_scores):.2%}\")\n",
    "#     accuracy = np.mean(accuracies)\n",
    "#     precision = np.mean(precisions)\n",
    "#     recall = np.mean(recalls)\n",
    "#     f1 = np.mean(f1_scores)\n",
    "#     return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# ##### SVM FUNCTION #####\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# def svm_pipeline(data, train_labels, test_labels, name, test_size=0.2):\n",
    "    \n",
    "#     train_data, test_data, train_labels, _ = train_test_split(data, train_labels, test_size=test_size, stratify=vote_labels, random_state=42)\n",
    "#     test_labels = test_labels[len(train_labels):]\n",
    "#     splits = [(range(len(train_data)), range(len(test_data)))]\n",
    "    \n",
    "#     accuracies = []\n",
    "#     precisions = []\n",
    "#     recalls = []\n",
    "#     f1_scores = []\n",
    "    \n",
    "#     # It's a binary classification, LabelEncoder is used instead of LabelBinarizer\n",
    "#     train_labels = np.argmax(train_labels, axis=1)\n",
    "#     test_labels = np.argmax(test_labels, axis=1)\n",
    "#     le = LabelEncoder()\n",
    "#     train_labels = le.fit_transform(train_labels)\n",
    "#     test_labels = le.transform(test_labels)\n",
    "\n",
    "#     #data = data.values\n",
    "#     # Define the pipeline\n",
    "#     pipeline = Pipeline([\n",
    "#         ('tfidf', TfidfVectorizer(max_df=0.9, min_df=5)),\n",
    "#         ('clf', SVC(kernel='linear', probability=True, class_weight='balanced'))\n",
    "#     ])\n",
    "\n",
    "#     # Train the SVM classifier\n",
    "#     pipeline.fit(train_data, train_labels)\n",
    "\n",
    "#     y_pred = pipeline.predict(test_data)\n",
    "\n",
    "#     #EVALUATION METRICS\n",
    "#     accuracy = accuracy_score(test_labels, y_pred)\n",
    "#     precision = precision_score(test_labels, y_pred)\n",
    "#     recall = recall_score(test_labels, y_pred)\n",
    "#     f1 = f1_score(test_labels, y_pred)\n",
    "    \n",
    "#     # Print the classification report\n",
    "#     print(classification_report(test_labels, y_pred))\n",
    "\n",
    "#     print(f\"Accuracy: {accuracy:.2%}\")\n",
    "#     print(f\"Precision: {precision:.2%}\")\n",
    "#     print(f\"Recall: {recall:.2%}\")\n",
    "#     print(f\"F1-score: {f1:.2%}\")\n",
    "\n",
    "#     return accuracy, precision, recall, f1\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "# # Hyperparameter optimizations\n",
    "# search_space = {\n",
    "#     'lstm_units': [32, 64, 128],\n",
    "#     'spatial_dropout_rate': [0.1, 0.2, 0.3],\n",
    "#     'dense_units': [16, 32, 64],\n",
    "#     'learning_rate': [1e-3, 1e-4],\n",
    "#     'batch_size': [16, 32],\n",
    "#     'epochs': [5, 10, 15], \n",
    "#     'smote' : [True, False], \n",
    "#     'use_kfold' : [True, False]\n",
    "# }\n",
    "\n",
    "# # Hyperparameter setup\n",
    "\n",
    "\n",
    "# def generate_combinations(search_space):\n",
    "#     keys, values = zip(*search_space.items())\n",
    "#     return [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "# combinations = generate_combinations(search_space)\n",
    "\n",
    "# # Hyperparamter optimization functie\n",
    "# def run_pipeline_with_combination(combination, data, train_labels, test_labels):\n",
    "#     return sentiment_analysis_pipeline(\n",
    "#         data=data,\n",
    "#         train_labels=train_labels,\n",
    "#         test_labels=test_labels,\n",
    "#         smote=combination['smote'],\n",
    "#         use_kfold=combination['use_kfold'],\n",
    "#         lstm_units=combination['lstm_units'],\n",
    "#         spatial_dropout_rate=combination['spatial_dropout_rate'],\n",
    "#         dense_units=combination['dense_units'],\n",
    "#         learning_rate=combination['learning_rate'],\n",
    "#         batch_size=combination['batch_size'],\n",
    "#         epochs=combination['epochs']\n",
    "#     )\n",
    "\n",
    "# # Define the filename for the CSV file\n",
    "# filename = 'experiment_results.csv'\n",
    "\n",
    "# # Define the header for the CSV file\n",
    "# header = ['Experiment', 'Parameters', 'F1-score']\n",
    "\n",
    "# # Initialize an empty list to store the results\n",
    "# results = []\n",
    "\n",
    "# # Hyperparameter optimization Experiment 1 \n",
    "# best_combination_exp_1 = None\n",
    "# best_f1_exp_1 = -1\n",
    "\n",
    "# for combination in combinations:\n",
    "#     f1_exp_1 = run_pipeline_with_combination(combination, data=pv['embeddings'], train_labels=vote_labels, test_labels=vote_labels)\n",
    "    \n",
    "#     if f1_exp_1 > best_f1_exp_1:\n",
    "#         best_f1_exp_1 = f1_exp_1\n",
    "#         best_combination_exp_1 = combination\n",
    "\n",
    "# print(f\"Best combination experiment 1: {best_combination_exp_1}\")\n",
    "# print(f\"Best F1-score experiment 1: {best_f1_exp_1}\")\n",
    "\n",
    "# # Add the results for Experiment 1 to the results list\n",
    "# results.append(('Experiment 1', best_combination_exp_1, best_f1_exp_1))\n",
    "\n",
    "# # Hyperparameter optimization Experiment 2 \n",
    "# best_combination_exp_2 = None\n",
    "# best_f1_exp_2 = -1\n",
    "\n",
    "# for combination in combinations:\n",
    "#     f1_exp_2 = run_pipeline_with_combination(combination, data=pv['embeddings'], train_labels=vader_labels, test_labels=vader_labels)\n",
    "    \n",
    "#     if f1_exp_2 > best_f1_exp_2:\n",
    "#         best_f1_exp_2 = f1_exp_2\n",
    "#         best_combination_exp_2 = combination\n",
    "\n",
    "# print(f\"Best combination experiment 2: {best_combination_exp_2}\")\n",
    "# print(f\"Best F1-score experiment 2: {best_f1_exp_2}\")\n",
    "\n",
    "# # Add the results for Experiment 2 to the results list\n",
    "# results.append(('Experiment 2', best_combination_exp_2, best_f1_exp_2))\n",
    "\n",
    "# # Hyperparameter optimization Experiment 3 \n",
    "# best_combination_exp_3 = None\n",
    "# best_f1_exp_3 = -1\n",
    "\n",
    "# for combination in combinations:\n",
    "#     f1_exp_3 = run_pipeline_with_combination(combination, data=pv['embeddings'], train_labels=vader_labels, test_labels=vote_labels)\n",
    "    \n",
    "#     if f1_exp_3 > best_f1_exp_3:\n",
    "#         best_f1_exp_3 = f1_exp_3\n",
    "#         best_combination_exp_3 = combination\n",
    "\n",
    "# print(f\"Best combination experiment 3: {best_combination_exp_3}\")\n",
    "# print(f\"Best F1-score experiment 3: {best_f1_exp_3}\")\n",
    "\n",
    "# # Add the results for Experiment 3 to the results list\n",
    "# results.append(('Experiment 3', best_combination_exp_3, best_f1_exp_3))\n",
    "\n",
    "# # Hyperparameter optimization Experiment 4 \n",
    "# best_combination_exp_4 = None\n",
    "# best_f1_exp_4 = -1\n",
    "\n",
    "# for combination in combinations:\n",
    "#     f1_exp_4 = run_pipeline_with_combination(combination, data=pv['embeddings'], train_labels=textblob_labels, test_labels=textblob_labels)\n",
    "    \n",
    "#     if f1_exp_4 > best_f1_exp_4:\n",
    "#         best_f1_exp_4 = f1_exp_4\n",
    "#         best_combination_exp_4 = combination\n",
    "\n",
    "# print(f\"Best combination experiment 4: {best_combination_exp_4}\")\n",
    "# print(f\"Best F1-score experiment 4: {best_f1_exp_4}\")\n",
    "\n",
    "# # Add the results for Experiment 4 to the results list\n",
    "# results.append(('Experiment 4', best_combination_exp_4, best_f1_exp_4))\n",
    "\n",
    "# # Hyperparameter optimization Experiment 5\n",
    "# best_combination_exp_5 = None\n",
    "# best_f1_exp_5 = -1\n",
    "\n",
    "# for combination in combinations:\n",
    "#     f1_exp_5 = run_pipeline_with_combination(combination, data=pv['embeddings'], train_labels=textblob_labels, test_labels=vote_labels)\n",
    "    \n",
    "#     if f1_exp_5 > best_f1_exp_5:\n",
    "#         best_f1_exp_5 = f1_exp_5\n",
    "#         best_combination_exp_5 = combination\n",
    "\n",
    "# print(f\"Best combination experiment 5: {best_combination_exp_5}\")\n",
    "# print(f\"Best F1-score experiment 5: {best_f1_exp_5}\")\n",
    "\n",
    "# # Add the results for Experiment 5 to the results list\n",
    "# results.append(('Experiment 5', best_combination_exp_5, best_f1_exp_5))\n",
    "\n",
    "# # Hyperparameter optimization Experiment 6 \n",
    "# best_combination_exp_6 = None\n",
    "# best_f1_exp_6 = -1\n",
    "\n",
    "# for combination in combinations:\n",
    "#     f1_exp_6 = run_pipeline_with_combination(combination, data=pv['embeddings'], train_labels=afinn_labels, test_labels=afinn_labels)\n",
    "    \n",
    "#     if f1_exp_6 > best_f1_exp_6:\n",
    "#         best_f1_exp_6 = f1_exp_6\n",
    "#         best_combination_exp_6 = combination\n",
    "\n",
    "# print(f\"Best combination experiment 6: {best_combination_exp_6}\")\n",
    "# print(f\"Best F1-score experiment 6: {best_f1_exp_6}\")\n",
    "\n",
    "# # Add the results for Experiment 6 to the results list\n",
    "# results.append(('Experiment 6', best_combination_exp_6, best_f1_exp_6))\n",
    "\n",
    "# # Hyperparameter optimization Experiment 7 \n",
    "# best_combination_exp_7 = None\n",
    "# best_f1_exp_7 = -1\n",
    "\n",
    "# for combination in combinations:\n",
    "#     f1_exp_7 = run_pipeline_with_combination(combination, data=pv['embeddings'], train_labels=afinn_labels, test_labels=vote_labels)\n",
    "    \n",
    "#     if f1_exp_7 > best_f1_exp_7:\n",
    "#         best_f1_exp_7 = f1_exp_7\n",
    "#         best_combination_exp_7 = combination\n",
    "\n",
    "# print(f\"Best combination experiment 7: {best_combination_exp_7}\")\n",
    "# print(f\"Best F1-score experiment 7: {best_f1_exp_7}\")\n",
    "\n",
    "# # Add the results for Experiment 7 to the results list\n",
    "# results.append(('Experiment 7', best_combination_exp_7, best_f1_exp_7))\n",
    "\n",
    "# # Write the results to the CSV file\n",
    "# with open(filename, 'a', newline='') as csvfile:\n",
    "#     writer = csv.writer(csvfile)\n",
    "#     writer.writerow(header)\n",
    "#     writer.writerows(results)\n",
    "\n",
    "\n",
    "###############################################################################################\n",
    "\n",
    "# Experiments with best parameters\n",
    "# Define the filename for the CSV file\n",
    "filename_2 = 'best_experiment_results.csv'\n",
    "\n",
    "# Define the header for the CSV file\n",
    "header_2 = ['Experiment', 'Model', 'Train label', 'Test label', 'Accuracy', 'Precision', 'Recall', 'F1', 'Majority baseline', 'Parameters']\n",
    "\n",
    "bilstm_parameters_human = \"kfold=False, LSTM_units = 64, \\n spatial_dropout = 0.3, \\n dense_units = 64, \\n 'learning_rate = 0.001, \\n batch_size=100, \\n epochs=10, \\n  input_dropout_rate=0.3, \\n recurrent_dropout_rate=0.3, \\n output_dropout_rate=0.4\"\n",
    "bilstm_parameters_auto = \"kfold=False, LSTM_units = 64, \\n spatial_dropout = 0.2, \\n dense_units = 64, \\n 'learning_rate = 0.001, \\n batch_size=100, \\n epochs=10, \\n  input_dropout_rate=0.2, \\n recurrent_dropout_rate=0.2, \\n output_dropout_rate=0.5\"\n",
    "bilstm_parameters_mix = \"kfold=False, LSTM_units = 64, \\n spatial_dropout = 0.3, \\n dense_units = 64, \\n 'learning_rate = 0.001, \\n batch_size=100, \\n epochs=10, \\n  input_dropout_rate=0.3, \\n recurrent_dropout_rate=0.3, \\n output_dropout_rate=0.5\"\n",
    "mlp_parameters = 'Standard'\n",
    "\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results_2 = []\n",
    "\n",
    "\n",
    "## Experiment 1 \n",
    "accuracy, precision, recall, f1 = sentiment_analysis_pipeline(\n",
    "        data=pv['embeddings'],\n",
    "        train_labels=vote_labels,\n",
    "        test_labels=vote_labels,\n",
    "        smote=False,\n",
    "        use_kfold=False,\n",
    "        lstm_units=64,\n",
    "        spatial_dropout_rate=0.3,\n",
    "        dense_units=64,\n",
    "        learning_rate=0.0001,\n",
    "        batch_size=100,\n",
    "        epochs=10,\n",
    "        input_dropout_rate=0.3, \n",
    "        recurrent_dropout_rate=0.3, \n",
    "        output_dropout_rate=0.4,\n",
    "        name='Experiment 1 BiLSTM'\n",
    ")\n",
    "\n",
    "\n",
    "# Add the results for Experiment 1 - BiLSTM to the results list\n",
    "results_2.append(('1', 'BiLSTM', 'Vote', 'Vote', accuracy, precision, recall, f1, vote_majority, bilstm_parameters_human))\n",
    "\n",
    "accuracy, precision, recall, f1 = mlp_baseline_pipeline(\n",
    "    data=pv['embeddings'],\n",
    "    train_labels=vote_labels,\n",
    "    test_labels=vote_labels,\n",
    "    test_size=0.2,\n",
    "    n_splits=5,  \n",
    "    learning_rate=0.0001,\n",
    "    batch_size=32,\n",
    "    epochs=10, \n",
    "    smote=False, \n",
    "    use_kfold=False, \n",
    "    name='Experiment 1 MLP'\n",
    ")\n",
    "\n",
    "\n",
    "# Add the results for Experiment 1 - MLP baseline to the results list\n",
    "results_2.append(('1', 'MLP', 'Vote', 'Vote', accuracy, precision, recall, f1, vote_majority, mlp_parameters))\n",
    "\n",
    "\n",
    "## Experiment 2\n",
    "accuracy, precision, recall, f1 = sentiment_analysis_pipeline(\n",
    "        data=pv['embeddings'],\n",
    "        train_labels=vader_labels,\n",
    "        test_labels=vader_labels,\n",
    "        smote=False,\n",
    "        use_kfold=False,\n",
    "        lstm_units=64,\n",
    "        spatial_dropout_rate=0.2,\n",
    "        dense_units=64,\n",
    "        learning_rate=0.001,\n",
    "        batch_size=100,\n",
    "        epochs=10,\n",
    "        input_dropout_rate=0.2, \n",
    "        recurrent_dropout_rate=0.2, \n",
    "        output_dropout_rate=0.5,\n",
    "        name='Experiment 2 BiLSTM'\n",
    ")\n",
    "\n",
    "\n",
    "# Add the results for Experiment 2 - BiLSTM to the results list\n",
    "results_2.append(('2', 'BiLSTM', 'VADER', 'VADER', accuracy, precision, recall, f1, vader_majority, bilstm_parameters_auto))\n",
    "\n",
    "accuracy, precision, recall, f1 = mlp_baseline_pipeline(\n",
    "    data=pv['embeddings'],\n",
    "    train_labels=vader_labels,\n",
    "    test_labels=vader_labels,\n",
    "    test_size=0.2,\n",
    "    n_splits=5,  \n",
    "    learning_rate=0.001,\n",
    "    batch_size=32,\n",
    "    epochs=10, \n",
    "    smote=False, \n",
    "    use_kfold=False,\n",
    "    name='Experiment 2 MLP'\n",
    ")\n",
    "\n",
    "\n",
    "# Add the results for Experiment 2 - MLP baseline to the results list\n",
    "results_2.append(('2', 'MLP', 'VADER', 'VADER', accuracy, precision, recall, f1, vader_majority, mlp_parameters))\n",
    "\n",
    "\n",
    "## Experiment 3 \n",
    "accuracy, precision, recall, f1 = sentiment_analysis_pipeline(\n",
    "        data=pv['embeddings'],\n",
    "        train_labels=vader_labels,\n",
    "        test_labels=vote_labels,\n",
    "        smote=False,\n",
    "        use_kfold=False,\n",
    "        lstm_units=64,\n",
    "        spatial_dropout_rate=0.3,\n",
    "        dense_units=64,\n",
    "        learning_rate=0.001,\n",
    "        batch_size=100,\n",
    "        epochs=10,\n",
    "        input_dropout_rate=0.3, \n",
    "        recurrent_dropout_rate=0.3, \n",
    "        output_dropout_rate=0.5,\n",
    "        name='Experiment 3 BiLSTM'\n",
    ")\n",
    "\n",
    "# Add the results for Experiment 3 - BiLSTM to the results list\n",
    "results_2.append(('3', 'BiLSTM', 'VADER', 'Vote', accuracy, precision, recall, f1, vote_majority, bilstm_parameters_mix))\n",
    "\n",
    "accuracy, precision, recall, f1 = mlp_baseline_pipeline(\n",
    "    data=pv['embeddings'],\n",
    "    train_labels=vader_labels,\n",
    "    test_labels=vote_labels,\n",
    "    test_size=0.2,\n",
    "    n_splits=5,  \n",
    "    learning_rate=0.001,\n",
    "    batch_size=32,\n",
    "    epochs=10, \n",
    "    smote=False, \n",
    "    use_kfold=False,\n",
    "    name='Experiment 3 MLP'\n",
    ")\n",
    "\n",
    "# Add the results for Experiment 3 - MLP baseline to the results list\n",
    "results_2.append(('3', 'MLP', 'VADER', 'Vote', accuracy, precision, recall, f1, vote_majority, mlp_parameters))\n",
    "\n",
    "\n",
    "## Experiment 4 \n",
    "accuracy, precision, recall, f1 = sentiment_analysis_pipeline(\n",
    "        data=pv['embeddings'],\n",
    "        train_labels=textblob_labels,\n",
    "        test_labels=textblob_labels,\n",
    "        smote=False,\n",
    "        use_kfold=False,\n",
    "        lstm_units=64,\n",
    "        spatial_dropout_rate=0.2,\n",
    "        dense_units=64,\n",
    "        learning_rate=0.001,\n",
    "        batch_size=100,\n",
    "        epochs=10,\n",
    "        input_dropout_rate=0.2, \n",
    "        recurrent_dropout_rate=0.2, \n",
    "        output_dropout_rate=0.5, \n",
    "        name='Experiment 4 BiLSTM'\n",
    ")\n",
    "\n",
    "# Add the results for Experiment 4 - BiLSTM to the results list\n",
    "results_2.append(('4', 'BiLSTM', 'TextBlob', 'TextBlob', accuracy, precision, recall, f1, textblob_majority, bilstm_parameters_auto))\n",
    "\n",
    "accuracy, precision, recall, f1 = mlp_baseline_pipeline(\n",
    "    data=pv['embeddings'],\n",
    "    train_labels=textblob_labels,\n",
    "    test_labels=textblob_labels,\n",
    "    test_size=0.2,\n",
    "    n_splits=5,  \n",
    "    learning_rate=0.001,\n",
    "    batch_size=32,\n",
    "    epochs=10, \n",
    "    smote=False, \n",
    "    use_kfold=False, \n",
    "    name='Experiment 4 MLP'\n",
    ")\n",
    "\n",
    "# Add the results for Experiment 4 - MLP baseline to the results list\n",
    "results_2.append(('4', 'MLP', 'TextBlob', 'TextBlob', accuracy, precision, recall, f1, textblob_majority, mlp_parameters))\n",
    "\n",
    "\n",
    "## Experiment 5 \n",
    "accuracy, precision, recall, f1 = sentiment_analysis_pipeline(\n",
    "        data=pv['embeddings'],\n",
    "        train_labels=textblob_labels,\n",
    "        test_labels=vote_labels,\n",
    "        smote=False,\n",
    "        use_kfold=False,\n",
    "        lstm_units=64,\n",
    "        spatial_dropout_rate=0.3,\n",
    "        dense_units=64,\n",
    "        learning_rate=0.001,\n",
    "        batch_size=100,\n",
    "        epochs=10,\n",
    "        input_dropout_rate=0.3, \n",
    "        recurrent_dropout_rate=0.3, \n",
    "        output_dropout_rate=0.5, \n",
    "        name='Experiment 5 BiLSTM'\n",
    ")\n",
    "\n",
    "# Add the results for Experiment 5 - BiLSTM to the results list\n",
    "results_2.append(('5', 'BiLSTM', 'TextBlob', 'Vote', accuracy, precision, recall, f1, vote_majority, bilstm_parameters_mix))\n",
    "\n",
    "accuracy, precision, recall, f1 = mlp_baseline_pipeline(\n",
    "    data=pv['embeddings'],\n",
    "    train_labels=textblob_labels,\n",
    "    test_labels=vote_labels,\n",
    "    test_size=0.2,\n",
    "    n_splits=5,  \n",
    "    learning_rate=0.001,\n",
    "    batch_size=32,\n",
    "    epochs=10, \n",
    "    smote=False, \n",
    "    use_kfold=False, \n",
    "    name='Experiment 5 MLP'\n",
    ")\n",
    "\n",
    "# Add the results for Experiment 5 - MLP baseline to the results list\n",
    "results_2.append(('5', 'MLP', 'TextBlob', 'Vote', accuracy, precision, recall, f1, vote_majority, mlp_parameters))\n",
    "\n",
    "\n",
    "## Experiment 6 \n",
    "accuracy, precision, recall, f1 = sentiment_analysis_pipeline(\n",
    "        data=pv['embeddings'],\n",
    "        train_labels=afinn_labels,\n",
    "        test_labels=afinn_labels,\n",
    "        smote=False,\n",
    "        use_kfold=False,\n",
    "        lstm_units=64,\n",
    "        spatial_dropout_rate=0.2,\n",
    "        dense_units=64,\n",
    "        learning_rate=0.001,\n",
    "        batch_size=100,\n",
    "        epochs=10,\n",
    "        input_dropout_rate=0.2, \n",
    "        recurrent_dropout_rate=0.2, \n",
    "        output_dropout_rate=0.5, \n",
    "        name='Experiment 6 BiLSTM'\n",
    ")\n",
    "\n",
    "# Add the results for Experiment 6 - BiLSTM to the results list\n",
    "results_2.append(('6', 'BiLSTM', 'Afinn', 'Afinn', accuracy, precision, recall, f1, afinn_majority, bilstm_parameters_auto))\n",
    "\n",
    "accuracy, precision, recall, f1 = mlp_baseline_pipeline(\n",
    "    data=pv['embeddings'],\n",
    "    train_labels=afinn_labels,\n",
    "    test_labels=afinn_labels,\n",
    "    test_size=0.2,\n",
    "    n_splits=5,  \n",
    "    learning_rate=0.001,\n",
    "    batch_size=32,\n",
    "    epochs=10, \n",
    "    smote=False, \n",
    "    use_kfold=False, \n",
    "    name='Experiment 6 MLP'\n",
    ")\n",
    "\n",
    "# Add the results for Experiment 6 - MLP baseline to the results list\n",
    "results_2.append(('6', 'MLP', 'Afinn', 'Afinn', accuracy, precision, recall, f1, afinn_majority, mlp_parameters))\n",
    "\n",
    "## Experiment 7 \n",
    "accuracy, precision, recall, f1 = sentiment_analysis_pipeline(\n",
    "        data=pv['embeddings'],\n",
    "        train_labels=afinn_labels,\n",
    "        test_labels=vote_labels,\n",
    "        smote=False,\n",
    "        use_kfold=False,\n",
    "        lstm_units=64,\n",
    "        spatial_dropout_rate=0.3,\n",
    "        dense_units=64,\n",
    "        learning_rate=0.001,\n",
    "        batch_size=100,\n",
    "        epochs=10,\n",
    "        input_dropout_rate=0.3, \n",
    "        recurrent_dropout_rate=0.3, \n",
    "        output_dropout_rate=0.5, \n",
    "        name='Experiment 7 BiLSTM'\n",
    ")\n",
    "\n",
    "# Add the results for Experiment 7 - BiLSTM to the results list\n",
    "results_2.append(('7', 'BiLSTM', 'Afinn', 'Vote', accuracy, precision, recall, f1, vote_majority, bilstm_parameters_mix))\n",
    "\n",
    "accuracy, precision, recall, f1 = mlp_baseline_pipeline(\n",
    "    data=pv['embeddings'],\n",
    "    train_labels=afinn_labels,\n",
    "    test_labels=vote_labels,\n",
    "    test_size=0.2,\n",
    "    n_splits=5,  \n",
    "    learning_rate=0.001,\n",
    "    batch_size=32,\n",
    "    epochs=10, \n",
    "    smote=False, \n",
    "    use_kfold=False, \n",
    "    name='Experiment 7 MLP'\n",
    ")\n",
    "\n",
    "# Add the results for Experiment 7 - MLP baseline to the results list\n",
    "results_2.append(('7', 'MLP', 'Afinn', 'Vote', accuracy, precision, recall, f1, vote_majority, mlp_parameters))\n",
    "\n",
    "## Experiment 8 \n",
    "accuracy, precision, recall, f1 = sentiment_analysis_pipeline(\n",
    "        data=pv['embeddings'],\n",
    "        train_labels=domain_labels,\n",
    "        test_labels=domain_labels,\n",
    "        smote=False,\n",
    "        use_kfold=False,\n",
    "        lstm_units=64,\n",
    "        spatial_dropout_rate=0.2,\n",
    "        dense_units=64,\n",
    "        learning_rate=0.001,\n",
    "        batch_size=100,\n",
    "        epochs=10,\n",
    "        input_dropout_rate=0.2, \n",
    "        recurrent_dropout_rate=0.2, \n",
    "        output_dropout_rate=0.5, \n",
    "        name='Experiment 8 BiLSTM'\n",
    ")\n",
    "\n",
    "# Add the results for Experiment 8 - BiLSTM to the results list\n",
    "results_2.append(('8', 'BiLSTM', 'Domain', 'Domain', accuracy, precision, recall, f1, domain_majority, bilstm_parameters_auto))\n",
    "\n",
    "accuracy, precision, recall, f1 = mlp_baseline_pipeline(\n",
    "    data=pv['embeddings'],\n",
    "    train_labels=domain_labels,\n",
    "    test_labels=domain_labels,\n",
    "    test_size=0.2,\n",
    "    n_splits=5,  \n",
    "    learning_rate=0.001,\n",
    "    batch_size=32,\n",
    "    epochs=10, \n",
    "    smote=False, \n",
    "    use_kfold=False, \n",
    "    name='Experiment 8 MLP'\n",
    ")\n",
    "\n",
    "# Add the results for Experiment 8 - MLP baseline to the results list\n",
    "results_2.append(('8', 'MLP', 'Domain', 'Domain', accuracy, precision, recall, f1, domain_majority, mlp_parameters))\n",
    "\n",
    "\n",
    "## Experiment 9 \n",
    "accuracy, precision, recall, f1 = sentiment_analysis_pipeline(\n",
    "        data=pv['embeddings'],\n",
    "        train_labels=domain_labels,\n",
    "        test_labels=vote_labels,\n",
    "        smote=False,\n",
    "        use_kfold=False,\n",
    "        lstm_units=64,\n",
    "        spatial_dropout_rate=0.3,\n",
    "        dense_units=64,\n",
    "        learning_rate=0.001,\n",
    "        batch_size=100,\n",
    "        epochs=10,\n",
    "        input_dropout_rate=0.3, \n",
    "        recurrent_dropout_rate=0.3, \n",
    "        output_dropout_rate=0.5, \n",
    "        name='Experiment 9 BiLSTM'\n",
    ")\n",
    "\n",
    "# Add the results for Experiment 9 - BiLSTM to the results list\n",
    "results_2.append(('9', 'BiLSTM', 'Domain', 'Vote', accuracy, precision, recall, f1, vote_majority, bilstm_parameters_mix))\n",
    "\n",
    "accuracy, precision, recall, f1 = mlp_baseline_pipeline(\n",
    "    data=pv['embeddings'],\n",
    "    train_labels=domain_labels,\n",
    "    test_labels=vote_labels,\n",
    "    test_size=0.2,\n",
    "    n_splits=5,  \n",
    "    learning_rate=0.001,\n",
    "    batch_size=32,\n",
    "    epochs=10, \n",
    "    smote=False, \n",
    "    use_kfold=False, \n",
    "    name='Experiment 9 MLP'\n",
    ")\n",
    "\n",
    "# Add the results for Experiment 9 - MLP baseline to the results list\n",
    "results_2.append(('9', 'MLP', 'Domain', 'Vote', accuracy, precision, recall, f1, vote_majority, mlp_parameters))\n",
    "\n",
    "\n",
    "## Write the results to the CSV file\n",
    "with open(filename_2, 'a', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header_2)\n",
    "    writer.writerows(results_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
